{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import visdom\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from modelas import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "#vis = visdom.Visdom()\n",
    "#vis.close(env=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_tracker(loss_plot, loss_value, num):\n",
    "    vis.line(X=num,\n",
    "            Y=loss_value,\n",
    "            win = loss_plot,\n",
    "            update = 'append'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "learning_rate = 0.00003\n",
    "training_epochs = 60\n",
    "batch_size = 256\n",
    "validation_split = .2\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(48, padding=4),\n",
    "    #transforms.Resize((32,32)),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(3),\n",
    "    #transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root='data/image',transform=transform_train)\n",
    "val_data = torchvision.datasets.ImageFolder(root='data/val',transform=transform_test)\n",
    "\n",
    "data_loader = DataLoader(dataset = train_data, batch_size = batch_size, num_workers = 2, shuffle=True)\n",
    "test_loader = DataLoader(dataset = val_data, batch_size=16,  num_workers=2, shuffle=True)\n",
    "\n",
    "classes = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingLoss(nn.Module):\n",
    "    def __init__(self, type='auto', loss_weight=1.0):\n",
    "        \"\"\"\n",
    "        :param type: type of loss ('l1', 'l2', 'auto')\n",
    "        :param loss_weight: weight of loss, for 'l1' and 'l2', try with 0.01. For 'auto', try with 1.0.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(RingLoss, self).__init__()\n",
    "        self.radius = Parameter(torch.Tensor(1))\n",
    "        self.radius.data.fill_(-1)\n",
    "        self.loss_weight = loss_weight\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.pow(2).sum(dim=1).pow(0.5)\n",
    "        if self.radius.data[0] < 0: # Initialize the radius with the mean feature norm of first iteration\n",
    "            self.radius.data.fill_(x.mean().data)\n",
    "        if self.type == 'l1': # Smooth L1 Loss\n",
    "            loss1 = F.smooth_l1_loss(x, self.radius.expand_as(x)).mul_(self.loss_weight)\n",
    "            loss2 = F.smooth_l1_loss(self.radius.expand_as(x), x).mul_(self.loss_weight)\n",
    "            ringloss = loss1 + loss2\n",
    "        elif self.type == 'auto': # Divide the L2 Loss by the feature's own norm\n",
    "            diff = x.sub(self.radius.expand_as(x)) / (x.mean().detach().clamp(min=0.5))\n",
    "            diff_sq = torch.pow(torch.abs(diff), 2).mean()\n",
    "            ringloss = diff_sq.mul_(self.loss_weight)\n",
    "        else: # L2 Loss, if not specified\n",
    "            diff = x.sub(self.radius.expand_as(x))\n",
    "            diff_sq = torch.pow(torch.abs(diff), 2).mean()\n",
    "            ringloss = diff_sq.mul_(self.loss_weight)\n",
    "        return ringloss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 5, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 1, 1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(48 ** 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(x.data.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        return self.fc2(x)\n",
    "\n",
    "net = Network()\n",
    "    \n",
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "# net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "# net = MobileNetV2()\n",
    "#et = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "# net = EfficientNetB0()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = RingLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#loss_plt = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='loss_tracker', legend=['loss'], showlegend=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "total_batch = len(data_loader)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    avg_cost = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        avg_cost +=loss / total_batch\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        #loss_tracker(loss_plt, torch.Tensor([train_loss/(batch_idx+1)]), torch.Tensor([batch_idx]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    print('Validation  Acc: %.3f%% (%d/%d)'\n",
    "          %(acc, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 1.593 | Acc: 31.250% (80/256)\n",
      "Loss: 1.597 | Acc: 30.273% (155/512)\n",
      "Loss: 1.600 | Acc: 29.167% (224/768)\n",
      "Loss: 1.603 | Acc: 28.125% (288/1024)\n",
      "Loss: 1.600 | Acc: 29.219% (374/1280)\n",
      "Loss: 1.601 | Acc: 29.036% (446/1536)\n",
      "Loss: 1.600 | Acc: 29.464% (528/1792)\n",
      "Loss: 1.599 | Acc: 30.127% (617/2048)\n",
      "Loss: 1.598 | Acc: 30.556% (704/2304)\n",
      "Loss: 1.598 | Acc: 30.391% (778/2560)\n",
      "Loss: 1.597 | Acc: 30.788% (867/2816)\n",
      "Loss: 1.597 | Acc: 30.729% (944/3072)\n",
      "Loss: 1.597 | Acc: 30.859% (1027/3328)\n",
      "Loss: 1.597 | Acc: 30.580% (1096/3584)\n",
      "Loss: 1.597 | Acc: 30.703% (1179/3840)\n",
      "Loss: 1.597 | Acc: 30.835% (1263/4096)\n",
      "Loss: 1.597 | Acc: 30.813% (1341/4352)\n",
      "Loss: 1.597 | Acc: 30.677% (1413/4606)\n",
      "Validation  Acc: 20.000% (40/200)\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.608 | Acc: 23.828% (61/256)\n",
      "Loss: 1.604 | Acc: 27.734% (142/512)\n",
      "Loss: 1.601 | Acc: 29.688% (228/768)\n",
      "Loss: 1.599 | Acc: 30.469% (312/1024)\n",
      "Loss: 1.600 | Acc: 29.766% (381/1280)\n",
      "Loss: 1.600 | Acc: 29.688% (456/1536)\n",
      "Loss: 1.600 | Acc: 30.022% (538/1792)\n",
      "Loss: 1.598 | Acc: 30.713% (629/2048)\n",
      "Loss: 1.597 | Acc: 30.642% (706/2304)\n",
      "Loss: 1.597 | Acc: 30.547% (782/2560)\n",
      "Loss: 1.597 | Acc: 31.143% (877/2816)\n",
      "Loss: 1.597 | Acc: 31.022% (953/3072)\n",
      "Loss: 1.598 | Acc: 30.739% (1023/3328)\n",
      "Loss: 1.598 | Acc: 30.636% (1098/3584)\n",
      "Loss: 1.598 | Acc: 30.547% (1173/3840)\n",
      "Loss: 1.598 | Acc: 30.444% (1247/4096)\n",
      "Loss: 1.597 | Acc: 30.836% (1342/4352)\n",
      "Loss: 1.597 | Acc: 30.764% (1417/4606)\n",
      "Validation  Acc: 20.000% (40/200)\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.600 | Acc: 27.734% (71/256)\n",
      "Loss: 1.597 | Acc: 29.297% (150/512)\n",
      "Loss: 1.598 | Acc: 30.078% (231/768)\n",
      "Loss: 1.597 | Acc: 31.152% (319/1024)\n",
      "Loss: 1.597 | Acc: 30.625% (392/1280)\n",
      "Loss: 1.598 | Acc: 29.753% (457/1536)\n",
      "Loss: 1.598 | Acc: 29.241% (524/1792)\n",
      "Loss: 1.597 | Acc: 30.078% (616/2048)\n",
      "Loss: 1.597 | Acc: 30.122% (694/2304)\n",
      "Loss: 1.596 | Acc: 30.547% (782/2560)\n",
      "Loss: 1.596 | Acc: 31.001% (873/2816)\n",
      "Loss: 1.595 | Acc: 31.152% (957/3072)\n",
      "Loss: 1.595 | Acc: 31.190% (1038/3328)\n",
      "Loss: 1.596 | Acc: 30.831% (1105/3584)\n",
      "Loss: 1.596 | Acc: 30.781% (1182/3840)\n",
      "Loss: 1.596 | Acc: 30.713% (1258/4096)\n",
      "Loss: 1.596 | Acc: 30.538% (1329/4352)\n",
      "Loss: 1.596 | Acc: 30.721% (1415/4606)\n",
      "Validation  Acc: 20.000% (40/200)\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.595 | Acc: 31.250% (80/256)\n",
      "Loss: 1.595 | Acc: 31.250% (160/512)\n",
      "Loss: 1.595 | Acc: 30.208% (232/768)\n",
      "Loss: 1.596 | Acc: 30.176% (309/1024)\n",
      "Loss: 1.599 | Acc: 28.672% (367/1280)\n",
      "Loss: 1.598 | Acc: 29.167% (448/1536)\n",
      "Loss: 1.597 | Acc: 29.688% (532/1792)\n",
      "Loss: 1.596 | Acc: 29.834% (611/2048)\n",
      "Loss: 1.597 | Acc: 29.557% (681/2304)\n",
      "Loss: 1.596 | Acc: 30.039% (769/2560)\n",
      "Loss: 1.596 | Acc: 30.220% (851/2816)\n",
      "Loss: 1.596 | Acc: 30.404% (934/3072)\n",
      "Loss: 1.595 | Acc: 30.709% (1022/3328)\n",
      "Loss: 1.595 | Acc: 30.915% (1108/3584)\n",
      "Loss: 1.595 | Acc: 31.094% (1194/3840)\n",
      "Loss: 1.595 | Acc: 31.128% (1275/4096)\n",
      "Loss: 1.595 | Acc: 31.020% (1350/4352)\n",
      "Loss: 1.595 | Acc: 30.873% (1422/4606)\n",
      "Validation  Acc: 20.000% (40/200)\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.594 | Acc: 31.250% (80/256)\n",
      "Loss: 1.593 | Acc: 30.859% (158/512)\n",
      "Loss: 1.593 | Acc: 29.948% (230/768)\n",
      "Loss: 1.596 | Acc: 29.395% (301/1024)\n",
      "Loss: 1.595 | Acc: 29.922% (383/1280)\n",
      "Loss: 1.593 | Acc: 30.599% (470/1536)\n",
      "Loss: 1.593 | Acc: 30.971% (555/1792)\n",
      "Loss: 1.593 | Acc: 30.811% (631/2048)\n",
      "Loss: 1.594 | Acc: 30.382% (700/2304)\n",
      "Loss: 1.593 | Acc: 30.703% (786/2560)\n",
      "Loss: 1.593 | Acc: 30.717% (865/2816)\n",
      "Loss: 1.594 | Acc: 30.566% (939/3072)\n",
      "Loss: 1.594 | Acc: 30.559% (1017/3328)\n",
      "Loss: 1.594 | Acc: 30.385% (1089/3584)\n",
      "Loss: 1.593 | Acc: 30.521% (1172/3840)\n",
      "Loss: 1.593 | Acc: 30.713% (1258/4096)\n",
      "Loss: 1.593 | Acc: 30.790% (1340/4352)\n",
      "Loss: 1.593 | Acc: 30.851% (1421/4606)\n",
      "Validation  Acc: 20.000% (40/200)\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.598 | Acc: 28.125% (72/256)\n",
      "Loss: 1.591 | Acc: 31.836% (163/512)\n",
      "Loss: 1.592 | Acc: 31.510% (242/768)\n",
      "Loss: 1.595 | Acc: 30.273% (310/1024)\n",
      "Loss: 1.594 | Acc: 29.609% (379/1280)\n",
      "Loss: 1.594 | Acc: 30.404% (467/1536)\n",
      "Loss: 1.594 | Acc: 30.078% (539/1792)\n",
      "Loss: 1.594 | Acc: 29.932% (613/2048)\n",
      "Loss: 1.594 | Acc: 29.818% (687/2304)\n",
      "Loss: 1.594 | Acc: 30.195% (773/2560)\n",
      "Loss: 1.593 | Acc: 30.682% (864/2816)\n",
      "Loss: 1.593 | Acc: 30.566% (939/3072)\n",
      "Loss: 1.594 | Acc: 30.228% (1006/3328)\n",
      "Loss: 1.593 | Acc: 30.497% (1093/3584)\n",
      "Loss: 1.593 | Acc: 30.625% (1176/3840)\n",
      "Loss: 1.593 | Acc: 30.518% (1250/4096)\n",
      "Loss: 1.593 | Acc: 30.744% (1338/4352)\n",
      "Loss: 1.592 | Acc: 31.046% (1430/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.589 | Acc: 31.250% (80/256)\n",
      "Loss: 1.594 | Acc: 29.688% (152/512)\n",
      "Loss: 1.590 | Acc: 31.380% (241/768)\n",
      "Loss: 1.588 | Acc: 32.422% (332/1024)\n",
      "Loss: 1.590 | Acc: 31.484% (403/1280)\n",
      "Loss: 1.591 | Acc: 30.990% (476/1536)\n",
      "Loss: 1.591 | Acc: 30.580% (548/1792)\n",
      "Loss: 1.591 | Acc: 30.566% (626/2048)\n",
      "Loss: 1.592 | Acc: 30.035% (692/2304)\n",
      "Loss: 1.592 | Acc: 30.039% (769/2560)\n",
      "Loss: 1.592 | Acc: 30.362% (855/2816)\n",
      "Loss: 1.592 | Acc: 30.241% (929/3072)\n",
      "Loss: 1.591 | Acc: 30.649% (1020/3328)\n",
      "Loss: 1.591 | Acc: 31.027% (1112/3584)\n",
      "Loss: 1.591 | Acc: 30.990% (1190/3840)\n",
      "Loss: 1.591 | Acc: 30.933% (1267/4096)\n",
      "Loss: 1.591 | Acc: 31.089% (1353/4352)\n",
      "Loss: 1.591 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1.583 | Acc: 33.594% (86/256)\n",
      "Loss: 1.588 | Acc: 31.836% (163/512)\n",
      "Loss: 1.589 | Acc: 30.859% (237/768)\n",
      "Loss: 1.588 | Acc: 31.543% (323/1024)\n",
      "Loss: 1.590 | Acc: 30.859% (395/1280)\n",
      "Loss: 1.590 | Acc: 30.794% (473/1536)\n",
      "Loss: 1.589 | Acc: 30.971% (555/1792)\n",
      "Loss: 1.590 | Acc: 30.908% (633/2048)\n",
      "Loss: 1.589 | Acc: 31.250% (720/2304)\n",
      "Loss: 1.590 | Acc: 30.742% (787/2560)\n",
      "Loss: 1.589 | Acc: 30.966% (872/2816)\n",
      "Loss: 1.589 | Acc: 30.892% (949/3072)\n",
      "Loss: 1.589 | Acc: 31.160% (1037/3328)\n",
      "Loss: 1.589 | Acc: 30.943% (1109/3584)\n",
      "Loss: 1.589 | Acc: 31.016% (1191/3840)\n",
      "Loss: 1.590 | Acc: 30.884% (1265/4096)\n",
      "Loss: 1.589 | Acc: 31.089% (1353/4352)\n",
      "Loss: 1.590 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 1.600 | Acc: 27.734% (71/256)\n",
      "Loss: 1.598 | Acc: 27.734% (142/512)\n",
      "Loss: 1.597 | Acc: 27.734% (213/768)\n",
      "Loss: 1.595 | Acc: 28.809% (295/1024)\n",
      "Loss: 1.593 | Acc: 29.844% (382/1280)\n",
      "Loss: 1.592 | Acc: 30.208% (464/1536)\n",
      "Loss: 1.591 | Acc: 30.580% (548/1792)\n",
      "Loss: 1.591 | Acc: 30.859% (632/2048)\n",
      "Loss: 1.590 | Acc: 31.120% (717/2304)\n",
      "Loss: 1.589 | Acc: 31.172% (798/2560)\n",
      "Loss: 1.589 | Acc: 31.179% (878/2816)\n",
      "Loss: 1.587 | Acc: 31.478% (967/3072)\n",
      "Loss: 1.587 | Acc: 31.400% (1045/3328)\n",
      "Loss: 1.587 | Acc: 31.194% (1118/3584)\n",
      "Loss: 1.587 | Acc: 31.146% (1196/3840)\n",
      "Loss: 1.587 | Acc: 31.274% (1281/4096)\n",
      "Loss: 1.588 | Acc: 31.181% (1357/4352)\n",
      "Loss: 1.588 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 1.597 | Acc: 26.953% (69/256)\n",
      "Loss: 1.584 | Acc: 32.227% (165/512)\n",
      "Loss: 1.586 | Acc: 31.771% (244/768)\n",
      "Loss: 1.587 | Acc: 32.129% (329/1024)\n",
      "Loss: 1.586 | Acc: 31.797% (407/1280)\n",
      "Loss: 1.586 | Acc: 32.161% (494/1536)\n",
      "Loss: 1.587 | Acc: 31.529% (565/1792)\n",
      "Loss: 1.586 | Acc: 31.885% (653/2048)\n",
      "Loss: 1.586 | Acc: 31.901% (735/2304)\n",
      "Loss: 1.585 | Acc: 31.914% (817/2560)\n",
      "Loss: 1.585 | Acc: 31.818% (896/2816)\n",
      "Loss: 1.586 | Acc: 31.608% (971/3072)\n",
      "Loss: 1.586 | Acc: 31.701% (1055/3328)\n",
      "Loss: 1.585 | Acc: 31.696% (1136/3584)\n",
      "Loss: 1.586 | Acc: 31.406% (1206/3840)\n",
      "Loss: 1.587 | Acc: 31.006% (1270/4096)\n",
      "Loss: 1.587 | Acc: 31.020% (1350/4352)\n",
      "Loss: 1.587 | Acc: 31.003% (1428/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 1.603 | Acc: 22.656% (58/256)\n",
      "Loss: 1.596 | Acc: 25.391% (130/512)\n",
      "Loss: 1.594 | Acc: 26.302% (202/768)\n",
      "Loss: 1.591 | Acc: 27.930% (286/1024)\n",
      "Loss: 1.589 | Acc: 29.297% (375/1280)\n",
      "Loss: 1.589 | Acc: 29.688% (456/1536)\n",
      "Loss: 1.587 | Acc: 30.078% (539/1792)\n",
      "Loss: 1.589 | Acc: 29.590% (606/2048)\n",
      "Loss: 1.588 | Acc: 29.991% (691/2304)\n",
      "Loss: 1.587 | Acc: 30.508% (781/2560)\n",
      "Loss: 1.586 | Acc: 31.072% (875/2816)\n",
      "Loss: 1.586 | Acc: 31.055% (954/3072)\n",
      "Loss: 1.586 | Acc: 31.010% (1032/3328)\n",
      "Loss: 1.586 | Acc: 31.027% (1112/3584)\n",
      "Loss: 1.586 | Acc: 30.964% (1189/3840)\n",
      "Loss: 1.586 | Acc: 31.055% (1272/4096)\n",
      "Loss: 1.586 | Acc: 31.066% (1352/4352)\n",
      "Loss: 1.586 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 1.581 | Acc: 33.203% (85/256)\n",
      "Loss: 1.583 | Acc: 32.617% (167/512)\n",
      "Loss: 1.585 | Acc: 32.031% (246/768)\n",
      "Loss: 1.583 | Acc: 32.324% (331/1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.582 | Acc: 32.500% (416/1280)\n",
      "Loss: 1.579 | Acc: 33.398% (513/1536)\n",
      "Loss: 1.580 | Acc: 32.757% (587/1792)\n",
      "Loss: 1.581 | Acc: 32.715% (670/2048)\n",
      "Loss: 1.581 | Acc: 32.509% (749/2304)\n",
      "Loss: 1.582 | Acc: 32.227% (825/2560)\n",
      "Loss: 1.583 | Acc: 31.889% (898/2816)\n",
      "Loss: 1.582 | Acc: 32.064% (985/3072)\n",
      "Loss: 1.583 | Acc: 31.731% (1056/3328)\n",
      "Loss: 1.582 | Acc: 31.780% (1139/3584)\n",
      "Loss: 1.583 | Acc: 31.406% (1206/3840)\n",
      "Loss: 1.583 | Acc: 31.567% (1293/4096)\n",
      "Loss: 1.583 | Acc: 31.273% (1361/4352)\n",
      "Loss: 1.584 | Acc: 31.090% (1432/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 1.588 | Acc: 30.078% (77/256)\n",
      "Loss: 1.589 | Acc: 30.273% (155/512)\n",
      "Loss: 1.590 | Acc: 29.948% (230/768)\n",
      "Loss: 1.590 | Acc: 28.613% (293/1024)\n",
      "Loss: 1.587 | Acc: 29.688% (380/1280)\n",
      "Loss: 1.587 | Acc: 29.818% (458/1536)\n",
      "Loss: 1.587 | Acc: 30.357% (544/1792)\n",
      "Loss: 1.587 | Acc: 30.420% (623/2048)\n",
      "Loss: 1.588 | Acc: 30.122% (694/2304)\n",
      "Loss: 1.587 | Acc: 30.586% (783/2560)\n",
      "Loss: 1.587 | Acc: 30.504% (859/2816)\n",
      "Loss: 1.586 | Acc: 30.827% (947/3072)\n",
      "Loss: 1.586 | Acc: 30.589% (1018/3328)\n",
      "Loss: 1.586 | Acc: 30.525% (1094/3584)\n",
      "Loss: 1.585 | Acc: 30.677% (1178/3840)\n",
      "Loss: 1.585 | Acc: 30.664% (1256/4096)\n",
      "Loss: 1.584 | Acc: 31.066% (1352/4352)\n",
      "Loss: 1.584 | Acc: 30.981% (1427/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 1.568 | Acc: 35.938% (92/256)\n",
      "Loss: 1.576 | Acc: 32.227% (165/512)\n",
      "Loss: 1.579 | Acc: 31.510% (242/768)\n",
      "Loss: 1.578 | Acc: 31.543% (323/1024)\n",
      "Loss: 1.581 | Acc: 30.781% (394/1280)\n",
      "Loss: 1.582 | Acc: 30.924% (475/1536)\n",
      "Loss: 1.583 | Acc: 30.748% (551/1792)\n",
      "Loss: 1.580 | Acc: 31.738% (650/2048)\n",
      "Loss: 1.581 | Acc: 31.510% (726/2304)\n",
      "Loss: 1.581 | Acc: 31.484% (806/2560)\n",
      "Loss: 1.581 | Acc: 31.889% (898/2816)\n",
      "Loss: 1.582 | Acc: 31.413% (965/3072)\n",
      "Loss: 1.582 | Acc: 31.160% (1037/3328)\n",
      "Loss: 1.583 | Acc: 30.971% (1110/3584)\n",
      "Loss: 1.583 | Acc: 31.146% (1196/3840)\n",
      "Loss: 1.582 | Acc: 31.079% (1273/4096)\n",
      "Loss: 1.582 | Acc: 31.112% (1354/4352)\n",
      "Loss: 1.582 | Acc: 31.046% (1430/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 1.579 | Acc: 32.031% (82/256)\n",
      "Loss: 1.581 | Acc: 31.445% (161/512)\n",
      "Loss: 1.587 | Acc: 29.948% (230/768)\n",
      "Loss: 1.585 | Acc: 30.566% (313/1024)\n",
      "Loss: 1.585 | Acc: 30.859% (395/1280)\n",
      "Loss: 1.586 | Acc: 30.469% (468/1536)\n",
      "Loss: 1.585 | Acc: 30.580% (548/1792)\n",
      "Loss: 1.584 | Acc: 30.713% (629/2048)\n",
      "Loss: 1.584 | Acc: 30.816% (710/2304)\n",
      "Loss: 1.584 | Acc: 30.938% (792/2560)\n",
      "Loss: 1.583 | Acc: 31.250% (880/2816)\n",
      "Loss: 1.582 | Acc: 31.250% (960/3072)\n",
      "Loss: 1.582 | Acc: 31.370% (1044/3328)\n",
      "Loss: 1.582 | Acc: 31.306% (1122/3584)\n",
      "Loss: 1.582 | Acc: 31.198% (1198/3840)\n",
      "Loss: 1.582 | Acc: 31.128% (1275/4096)\n",
      "Loss: 1.582 | Acc: 31.043% (1351/4352)\n",
      "Loss: 1.582 | Acc: 31.046% (1430/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 15\n",
      "Loss: 1.578 | Acc: 30.859% (79/256)\n",
      "Loss: 1.578 | Acc: 31.641% (162/512)\n",
      "Loss: 1.581 | Acc: 30.599% (235/768)\n",
      "Loss: 1.580 | Acc: 30.859% (316/1024)\n",
      "Loss: 1.582 | Acc: 30.547% (391/1280)\n",
      "Loss: 1.580 | Acc: 31.250% (480/1536)\n",
      "Loss: 1.580 | Acc: 30.915% (554/1792)\n",
      "Loss: 1.580 | Acc: 31.201% (639/2048)\n",
      "Loss: 1.579 | Acc: 31.207% (719/2304)\n",
      "Loss: 1.580 | Acc: 30.938% (792/2560)\n",
      "Loss: 1.580 | Acc: 31.108% (876/2816)\n",
      "Loss: 1.580 | Acc: 30.859% (948/3072)\n",
      "Loss: 1.580 | Acc: 30.889% (1028/3328)\n",
      "Loss: 1.580 | Acc: 31.138% (1116/3584)\n",
      "Loss: 1.580 | Acc: 30.964% (1189/3840)\n",
      "Loss: 1.581 | Acc: 30.664% (1256/4096)\n",
      "Loss: 1.581 | Acc: 30.699% (1336/4352)\n",
      "Loss: 1.580 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 16\n",
      "Loss: 1.590 | Acc: 26.172% (67/256)\n",
      "Loss: 1.580 | Acc: 29.883% (153/512)\n",
      "Loss: 1.580 | Acc: 30.078% (231/768)\n",
      "Loss: 1.582 | Acc: 29.883% (306/1024)\n",
      "Loss: 1.582 | Acc: 29.844% (382/1280)\n",
      "Loss: 1.580 | Acc: 30.339% (466/1536)\n",
      "Loss: 1.581 | Acc: 30.078% (539/1792)\n",
      "Loss: 1.583 | Acc: 29.736% (609/2048)\n",
      "Loss: 1.581 | Acc: 30.252% (697/2304)\n",
      "Loss: 1.581 | Acc: 30.273% (775/2560)\n",
      "Loss: 1.581 | Acc: 30.185% (850/2816)\n",
      "Loss: 1.580 | Acc: 30.469% (936/3072)\n",
      "Loss: 1.581 | Acc: 30.228% (1006/3328)\n",
      "Loss: 1.581 | Acc: 30.552% (1095/3584)\n",
      "Loss: 1.581 | Acc: 30.547% (1173/3840)\n",
      "Loss: 1.580 | Acc: 30.737% (1259/4096)\n",
      "Loss: 1.580 | Acc: 30.790% (1340/4352)\n",
      "Loss: 1.579 | Acc: 31.046% (1430/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 17\n",
      "Loss: 1.578 | Acc: 32.031% (82/256)\n",
      "Loss: 1.581 | Acc: 30.859% (158/512)\n",
      "Loss: 1.581 | Acc: 30.729% (236/768)\n",
      "Loss: 1.580 | Acc: 31.152% (319/1024)\n",
      "Loss: 1.583 | Acc: 30.312% (388/1280)\n",
      "Loss: 1.584 | Acc: 29.948% (460/1536)\n",
      "Loss: 1.582 | Acc: 30.525% (547/1792)\n",
      "Loss: 1.581 | Acc: 30.469% (624/2048)\n",
      "Loss: 1.581 | Acc: 30.642% (706/2304)\n",
      "Loss: 1.580 | Acc: 30.820% (789/2560)\n",
      "Loss: 1.580 | Acc: 30.824% (868/2816)\n",
      "Loss: 1.579 | Acc: 31.055% (954/3072)\n",
      "Loss: 1.580 | Acc: 30.829% (1026/3328)\n",
      "Loss: 1.580 | Acc: 30.887% (1107/3584)\n",
      "Loss: 1.578 | Acc: 31.146% (1196/3840)\n",
      "Loss: 1.579 | Acc: 30.933% (1267/4096)\n",
      "Loss: 1.579 | Acc: 30.630% (1333/4352)\n",
      "Loss: 1.578 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 18\n",
      "Loss: 1.579 | Acc: 29.688% (76/256)\n",
      "Loss: 1.575 | Acc: 31.641% (162/512)\n",
      "Loss: 1.579 | Acc: 30.078% (231/768)\n",
      "Loss: 1.581 | Acc: 29.883% (306/1024)\n",
      "Loss: 1.579 | Acc: 30.625% (392/1280)\n",
      "Loss: 1.578 | Acc: 30.859% (474/1536)\n",
      "Loss: 1.579 | Acc: 30.469% (546/1792)\n",
      "Loss: 1.579 | Acc: 30.566% (626/2048)\n",
      "Loss: 1.580 | Acc: 30.295% (698/2304)\n",
      "Loss: 1.580 | Acc: 30.508% (781/2560)\n",
      "Loss: 1.580 | Acc: 30.504% (859/2816)\n",
      "Loss: 1.578 | Acc: 30.892% (949/3072)\n",
      "Loss: 1.579 | Acc: 30.799% (1025/3328)\n",
      "Loss: 1.579 | Acc: 30.720% (1101/3584)\n",
      "Loss: 1.580 | Acc: 30.547% (1173/3840)\n",
      "Loss: 1.579 | Acc: 30.762% (1260/4096)\n",
      "Loss: 1.578 | Acc: 31.043% (1351/4352)\n",
      "Loss: 1.578 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 19\n",
      "Loss: 1.584 | Acc: 29.297% (75/256)\n",
      "Loss: 1.585 | Acc: 28.320% (145/512)\n",
      "Loss: 1.583 | Acc: 28.125% (216/768)\n",
      "Loss: 1.578 | Acc: 29.785% (305/1024)\n",
      "Loss: 1.580 | Acc: 29.609% (379/1280)\n",
      "Loss: 1.580 | Acc: 29.948% (460/1536)\n",
      "Loss: 1.579 | Acc: 29.855% (535/1792)\n",
      "Loss: 1.578 | Acc: 30.127% (617/2048)\n",
      "Loss: 1.578 | Acc: 30.208% (696/2304)\n",
      "Loss: 1.577 | Acc: 30.859% (790/2560)\n",
      "Loss: 1.576 | Acc: 31.072% (875/2816)\n",
      "Loss: 1.576 | Acc: 31.120% (956/3072)\n",
      "Loss: 1.576 | Acc: 31.160% (1037/3328)\n",
      "Loss: 1.576 | Acc: 31.110% (1115/3584)\n",
      "Loss: 1.576 | Acc: 31.224% (1199/3840)\n",
      "Loss: 1.576 | Acc: 31.323% (1283/4096)\n",
      "Loss: 1.577 | Acc: 31.181% (1357/4352)\n",
      "Loss: 1.577 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 20\n",
      "Loss: 1.580 | Acc: 29.688% (76/256)\n",
      "Loss: 1.583 | Acc: 27.734% (142/512)\n",
      "Loss: 1.576 | Acc: 30.339% (233/768)\n",
      "Loss: 1.576 | Acc: 30.859% (316/1024)\n",
      "Loss: 1.575 | Acc: 30.938% (396/1280)\n",
      "Loss: 1.574 | Acc: 31.576% (485/1536)\n",
      "Loss: 1.576 | Acc: 31.083% (557/1792)\n",
      "Loss: 1.577 | Acc: 30.615% (627/2048)\n",
      "Loss: 1.576 | Acc: 30.859% (711/2304)\n",
      "Loss: 1.575 | Acc: 31.211% (799/2560)\n",
      "Loss: 1.576 | Acc: 31.250% (880/2816)\n",
      "Loss: 1.577 | Acc: 30.859% (948/3072)\n",
      "Loss: 1.577 | Acc: 30.980% (1031/3328)\n",
      "Loss: 1.577 | Acc: 30.999% (1111/3584)\n",
      "Loss: 1.576 | Acc: 30.964% (1189/3840)\n",
      "Loss: 1.576 | Acc: 30.786% (1261/4096)\n",
      "Loss: 1.576 | Acc: 30.951% (1347/4352)\n",
      "Loss: 1.576 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 21\n",
      "Loss: 1.578 | Acc: 27.734% (71/256)\n",
      "Loss: 1.577 | Acc: 30.273% (155/512)\n",
      "Loss: 1.575 | Acc: 31.901% (245/768)\n",
      "Loss: 1.574 | Acc: 31.836% (326/1024)\n",
      "Loss: 1.574 | Acc: 31.953% (409/1280)\n",
      "Loss: 1.573 | Acc: 31.966% (491/1536)\n",
      "Loss: 1.573 | Acc: 31.752% (569/1792)\n",
      "Loss: 1.573 | Acc: 31.592% (647/2048)\n",
      "Loss: 1.573 | Acc: 31.727% (731/2304)\n",
      "Loss: 1.575 | Acc: 31.133% (797/2560)\n",
      "Loss: 1.575 | Acc: 31.143% (877/2816)\n",
      "Loss: 1.576 | Acc: 30.990% (952/3072)\n",
      "Loss: 1.576 | Acc: 30.950% (1030/3328)\n",
      "Loss: 1.575 | Acc: 31.194% (1118/3584)\n",
      "Loss: 1.575 | Acc: 31.146% (1196/3840)\n",
      "Loss: 1.576 | Acc: 31.030% (1271/4096)\n",
      "Loss: 1.576 | Acc: 31.020% (1350/4352)\n",
      "Loss: 1.576 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 22\n",
      "Loss: 1.575 | Acc: 32.031% (82/256)\n",
      "Loss: 1.575 | Acc: 30.859% (158/512)\n",
      "Loss: 1.571 | Acc: 32.552% (250/768)\n",
      "Loss: 1.575 | Acc: 31.445% (322/1024)\n",
      "Loss: 1.576 | Acc: 30.781% (394/1280)\n",
      "Loss: 1.573 | Acc: 31.771% (488/1536)\n",
      "Loss: 1.574 | Acc: 31.752% (569/1792)\n",
      "Loss: 1.575 | Acc: 31.299% (641/2048)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.574 | Acc: 31.641% (729/2304)\n",
      "Loss: 1.574 | Acc: 31.797% (814/2560)\n",
      "Loss: 1.575 | Acc: 31.392% (884/2816)\n",
      "Loss: 1.575 | Acc: 31.413% (965/3072)\n",
      "Loss: 1.575 | Acc: 31.400% (1045/3328)\n",
      "Loss: 1.574 | Acc: 31.501% (1129/3584)\n",
      "Loss: 1.574 | Acc: 31.380% (1205/3840)\n",
      "Loss: 1.575 | Acc: 31.104% (1274/4096)\n",
      "Loss: 1.575 | Acc: 30.905% (1345/4352)\n",
      "Loss: 1.575 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 23\n",
      "Loss: 1.578 | Acc: 31.250% (80/256)\n",
      "Loss: 1.575 | Acc: 31.250% (160/512)\n",
      "Loss: 1.575 | Acc: 32.161% (247/768)\n",
      "Loss: 1.576 | Acc: 31.543% (323/1024)\n",
      "Loss: 1.572 | Acc: 32.422% (415/1280)\n",
      "Loss: 1.573 | Acc: 32.096% (493/1536)\n",
      "Loss: 1.570 | Acc: 32.589% (584/1792)\n",
      "Loss: 1.572 | Acc: 32.129% (658/2048)\n",
      "Loss: 1.570 | Acc: 32.378% (746/2304)\n",
      "Loss: 1.572 | Acc: 31.875% (816/2560)\n",
      "Loss: 1.572 | Acc: 31.570% (889/2816)\n",
      "Loss: 1.573 | Acc: 31.348% (963/3072)\n",
      "Loss: 1.573 | Acc: 31.430% (1046/3328)\n",
      "Loss: 1.573 | Acc: 31.306% (1122/3584)\n",
      "Loss: 1.573 | Acc: 31.094% (1194/3840)\n",
      "Loss: 1.573 | Acc: 31.055% (1272/4096)\n",
      "Loss: 1.574 | Acc: 30.974% (1348/4352)\n",
      "Loss: 1.573 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 24\n",
      "Loss: 1.582 | Acc: 30.859% (79/256)\n",
      "Loss: 1.574 | Acc: 31.641% (162/512)\n",
      "Loss: 1.572 | Acc: 32.292% (248/768)\n",
      "Loss: 1.571 | Acc: 32.422% (332/1024)\n",
      "Loss: 1.573 | Acc: 31.953% (409/1280)\n",
      "Loss: 1.573 | Acc: 31.315% (481/1536)\n",
      "Loss: 1.572 | Acc: 31.362% (562/1792)\n",
      "Loss: 1.573 | Acc: 31.152% (638/2048)\n",
      "Loss: 1.573 | Acc: 31.207% (719/2304)\n",
      "Loss: 1.574 | Acc: 30.703% (786/2560)\n",
      "Loss: 1.572 | Acc: 31.179% (878/2816)\n",
      "Loss: 1.572 | Acc: 31.120% (956/3072)\n",
      "Loss: 1.572 | Acc: 30.980% (1031/3328)\n",
      "Loss: 1.572 | Acc: 31.138% (1116/3584)\n",
      "Loss: 1.572 | Acc: 31.016% (1191/3840)\n",
      "Loss: 1.572 | Acc: 31.274% (1281/4096)\n",
      "Loss: 1.572 | Acc: 31.158% (1356/4352)\n",
      "Loss: 1.572 | Acc: 31.046% (1430/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 25\n",
      "Loss: 1.561 | Acc: 34.375% (88/256)\n",
      "Loss: 1.571 | Acc: 32.031% (164/512)\n",
      "Loss: 1.572 | Acc: 31.510% (242/768)\n",
      "Loss: 1.570 | Acc: 32.715% (335/1024)\n",
      "Loss: 1.569 | Acc: 32.812% (420/1280)\n",
      "Loss: 1.570 | Acc: 32.487% (499/1536)\n",
      "Loss: 1.571 | Acc: 32.199% (577/1792)\n",
      "Loss: 1.571 | Acc: 32.031% (656/2048)\n",
      "Loss: 1.571 | Acc: 31.684% (730/2304)\n",
      "Loss: 1.570 | Acc: 31.797% (814/2560)\n",
      "Loss: 1.571 | Acc: 31.463% (886/2816)\n",
      "Loss: 1.571 | Acc: 31.283% (961/3072)\n",
      "Loss: 1.572 | Acc: 31.250% (1040/3328)\n",
      "Loss: 1.572 | Acc: 31.306% (1122/3584)\n",
      "Loss: 1.573 | Acc: 30.990% (1190/3840)\n",
      "Loss: 1.573 | Acc: 30.908% (1266/4096)\n",
      "Loss: 1.572 | Acc: 31.181% (1357/4352)\n",
      "Loss: 1.572 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 26\n",
      "Loss: 1.594 | Acc: 24.219% (62/256)\n",
      "Loss: 1.583 | Acc: 27.930% (143/512)\n",
      "Loss: 1.580 | Acc: 29.036% (223/768)\n",
      "Loss: 1.579 | Acc: 29.395% (301/1024)\n",
      "Loss: 1.575 | Acc: 30.078% (385/1280)\n",
      "Loss: 1.575 | Acc: 29.883% (459/1536)\n",
      "Loss: 1.574 | Acc: 30.022% (538/1792)\n",
      "Loss: 1.574 | Acc: 30.029% (615/2048)\n",
      "Loss: 1.572 | Acc: 30.599% (705/2304)\n",
      "Loss: 1.571 | Acc: 30.625% (784/2560)\n",
      "Loss: 1.572 | Acc: 30.575% (861/2816)\n",
      "Loss: 1.571 | Acc: 30.924% (950/3072)\n",
      "Loss: 1.572 | Acc: 30.559% (1017/3328)\n",
      "Loss: 1.573 | Acc: 30.385% (1089/3584)\n",
      "Loss: 1.572 | Acc: 30.625% (1176/3840)\n",
      "Loss: 1.572 | Acc: 30.811% (1262/4096)\n",
      "Loss: 1.572 | Acc: 30.836% (1342/4352)\n",
      "Loss: 1.571 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 27\n",
      "Loss: 1.544 | Acc: 38.281% (98/256)\n",
      "Loss: 1.565 | Acc: 31.641% (162/512)\n",
      "Loss: 1.568 | Acc: 31.641% (243/768)\n",
      "Loss: 1.566 | Acc: 32.324% (331/1024)\n",
      "Loss: 1.568 | Acc: 31.953% (409/1280)\n",
      "Loss: 1.567 | Acc: 32.096% (493/1536)\n",
      "Loss: 1.566 | Acc: 32.310% (579/1792)\n",
      "Loss: 1.567 | Acc: 31.982% (655/2048)\n",
      "Loss: 1.568 | Acc: 31.858% (734/2304)\n",
      "Loss: 1.567 | Acc: 31.953% (818/2560)\n",
      "Loss: 1.568 | Acc: 31.641% (891/2816)\n",
      "Loss: 1.569 | Acc: 31.510% (968/3072)\n",
      "Loss: 1.569 | Acc: 31.190% (1038/3328)\n",
      "Loss: 1.570 | Acc: 30.887% (1107/3584)\n",
      "Loss: 1.569 | Acc: 31.120% (1195/3840)\n",
      "Loss: 1.569 | Acc: 31.006% (1270/4096)\n",
      "Loss: 1.570 | Acc: 30.859% (1343/4352)\n",
      "Loss: 1.570 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 28\n",
      "Loss: 1.584 | Acc: 27.734% (71/256)\n",
      "Loss: 1.583 | Acc: 26.953% (138/512)\n",
      "Loss: 1.577 | Acc: 29.297% (225/768)\n",
      "Loss: 1.572 | Acc: 30.371% (311/1024)\n",
      "Loss: 1.567 | Acc: 31.719% (406/1280)\n",
      "Loss: 1.571 | Acc: 30.794% (473/1536)\n",
      "Loss: 1.570 | Acc: 30.692% (550/1792)\n",
      "Loss: 1.572 | Acc: 30.615% (627/2048)\n",
      "Loss: 1.572 | Acc: 30.642% (706/2304)\n",
      "Loss: 1.571 | Acc: 30.742% (787/2560)\n",
      "Loss: 1.572 | Acc: 30.611% (862/2816)\n",
      "Loss: 1.571 | Acc: 31.087% (955/3072)\n",
      "Loss: 1.571 | Acc: 30.919% (1029/3328)\n",
      "Loss: 1.571 | Acc: 31.083% (1114/3584)\n",
      "Loss: 1.571 | Acc: 31.094% (1194/3840)\n",
      "Loss: 1.570 | Acc: 31.226% (1279/4096)\n",
      "Loss: 1.570 | Acc: 31.250% (1360/4352)\n",
      "Loss: 1.571 | Acc: 31.025% (1429/4606)\n",
      "Validation  Acc: 20.500% (41/200)\n",
      "\n",
      "Epoch: 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-60f6814b7fac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-33889fc583b1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mavg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    866\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,200):\n",
    "    train(i)\n",
    "    test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
